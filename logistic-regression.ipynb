{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9393137,"sourceType":"datasetVersion","datasetId":5700459},{"sourceId":5034040,"sourceType":"datasetVersion","datasetId":2921725}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We are about to do the Logistic Regression, one of the basic statistical methods. Even though the name says \"Regression\", its actually a binary classification. So the output is yes or no, 1 or 0, cat or not cat, delay or no delay etc. ","metadata":{}},{"cell_type":"markdown","source":"To out learning curve smooth, lets start with something simple.\nLets work with a simple dataset.\nAnd as always, at first we import the essential libraries, Pandas and NumPy.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T18:40:21.897852Z","iopub.execute_input":"2025-06-19T18:40:21.898149Z","iopub.status.idle":"2025-06-19T18:40:22.208158Z","shell.execute_reply.started":"2025-06-19T18:40:21.898115Z","shell.execute_reply":"2025-06-19T18:40:22.207348Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### **Toy Dataset: Exam Performance Prediction (Binary Classification)**\n**Description**: Predict whether a student passes (`1`) or fails (`0`) based on study hours and sleep hours.\n\n```csv\nStudy_Hours,Sleep_Hours,Pass\n1.0,4.0,0\n1.5,5.0,0\n2.0,6.0,0\n2.5,4.5,0\n3.0,5.5,0\n3.5,7.0,1\n4.0,6.5,1\n4.5,7.5,1\n5.0,8.0,1\n5.5,7.0,1\n6.0,5.0,0\n6.5,8.5,1\n7.0,9.0,1\n7.5,6.0,1\n8.0,9.5,1\n8.5,7.5,1\n9.0,10.0,1\n9.5,5.5,1\n10.0,8.5,1\n10.5,9.0,1\n```\n\n#### **Columns**\n- `Study_Hours`: Hours studied (numerical, range: 1.0–10.5)  \n- `Sleep_Hours`: Hours slept (numerical, range: 4.0–10.0)  \n- `Pass`: Binary label (`0` = Fail, `1` = Pass)  \n\nLets visualize the data here, to see how it talks to us: ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Features (Study Hours, Sleep Hours)\nX = np.array([\n    [1.0, 4.0], [1.5, 5.0], [2.0, 6.0], [2.5, 4.5], [3.0, 5.5],\n    [3.5, 7.0], [4.0, 6.5], [4.5, 7.5], [5.0, 8.0], [5.5, 7.0],\n    [6.0, 5.0], [6.5, 8.5], [7.0, 9.0], [7.5, 6.0], [8.0, 9.5],\n    [8.5, 7.5], [9.0, 10.0], [9.5, 5.5], [10.0, 8.5], [10.5, 9.0]\n])\n\n# Labels (0 = Fail, 1 = Pass)\ny = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\nprint(\"Features (X):\\n\", X)\nprint(\"Labels (y):\", y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:54.605441Z","iopub.execute_input":"2025-06-10T17:38:54.605715Z","iopub.status.idle":"2025-06-10T17:38:54.613185Z","shell.execute_reply.started":"2025-06-10T17:38:54.605693Z","shell.execute_reply":"2025-06-10T17:38:54.612458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame({\n    'Study_Hours': X[:, 0],  # First column of X\n    'Sleep_Hours': X[:, 1],  # Second column of X\n    'Pass': y\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:54.637043Z","iopub.execute_input":"2025-06-10T17:38:54.637665Z","iopub.status.idle":"2025-06-10T17:38:54.642863Z","shell.execute_reply.started":"2025-06-10T17:38:54.637638Z","shell.execute_reply":"2025-06-10T17:38:54.641582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', label='Fail')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color='green', label='Pass')\nplt.xlabel('Study Hours')\nplt.ylabel('Sleep Hours')\nplt.legend()\nplt.title('Exam Performance Dataset')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:54.669652Z","iopub.execute_input":"2025-06-10T17:38:54.669959Z","iopub.status.idle":"2025-06-10T17:38:54.883593Z","shell.execute_reply.started":"2025-06-10T17:38:54.669934Z","shell.execute_reply":"2025-06-10T17:38:54.882459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nsns.scatterplot(data=df, x='Study_Hours', y='Sleep_Hours', hue='Pass', palette=['red', 'green'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:54.885066Z","iopub.execute_input":"2025-06-10T17:38:54.885398Z","iopub.status.idle":"2025-06-10T17:38:55.129925Z","shell.execute_reply.started":"2025-06-10T17:38:54.885364Z","shell.execute_reply":"2025-06-10T17:38:55.128950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hmm, it's pretty easy. Because it's made up. We can almost divide the 0 and 1 simply by drawing a line between them. Lets see how that works. ","metadata":{}},{"cell_type":"markdown","source":"As we said before, we can try to draw a straight line and separate the reds from greens. Lets take that approach.\nIf we want to draw a line, we need a line's equation. The simplest equation we have is:\n\n$$Y = ax + b$$\n\nwhere, \"a\" is the slope of \"x\", the input variable. And \"b\" is the bias. \"a\" decide the rate of change in Y from x, and \"b\" is the headstart, or how much elevation we get.\nNow, we need to find Y, and Y isn't exactly ax + b. We have to apply a function to get Y. It's more like $$Y = some_function(ax + b)$$\nLet's take a another variable z, z = ax+b. Later we can get the value of Y from z.  \nOne more problem, we have two different values of x here. The \"study hours\" and the \"sleep hours\".\nWe can work with that. Essentialy, we have to assume that study hours and sleep hours dont affect result similarly. Probably study hour is more important? And sleep hour is not that important? But we can not run on 0 hour of sleep, can we?\nSo, these two independant variable, x1 (study hour), and x2(sleep hour) should be \"weighted\" differently. \nHere, weights = [w1, w2]\nSo, $$z = w1 * x1 + w2 * x2 + b$$","metadata":{}},{"cell_type":"code","source":"X_bias = np.c_[np.ones((X.shape[0], 1)), X]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:55.130787Z","iopub.execute_input":"2025-06-10T17:38:55.131112Z","iopub.status.idle":"2025-06-10T17:38:55.135416Z","shell.execute_reply.started":"2025-06-10T17:38:55.131089Z","shell.execute_reply":"2025-06-10T17:38:55.134548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"X_bias is an array that consist of b, x1, x2.","metadata":{}},{"cell_type":"markdown","source":"We are going to split the dataset into train and test part. Our regression will learn from train set, and test the hypothesis on the test set.\nWe are doing 80% 20% split here.","metadata":{}},{"cell_type":"code","source":"np.random.shuffle(X_bias) #shuffling the dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:55.137539Z","iopub.execute_input":"2025-06-10T17:38:55.138310Z","iopub.status.idle":"2025-06-10T17:38:55.153353Z","shell.execute_reply.started":"2025-06-10T17:38:55.138281Z","shell.execute_reply":"2025-06-10T17:38:55.152405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size = int(len(X_bias) * 0.8)  # 80% of the total length\ntrain_set = X_bias[:train_size]  # First 80% of elements\ntest_set = X_bias[train_size:]  # Remaining 20% of elements","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:55.154173Z","iopub.execute_input":"2025-06-10T17:38:55.154458Z","iopub.status.idle":"2025-06-10T17:38:55.170151Z","shell.execute_reply.started":"2025-06-10T17:38:55.154430Z","shell.execute_reply":"2025-06-10T17:38:55.169217Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We are initializing weight array.","metadata":{}},{"cell_type":"code","source":"w = np.zeros(X_bias.shape[1])  # Shape: (3,) for [bias, w1, w2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:55.171043Z","iopub.execute_input":"2025-06-10T17:38:55.171370Z","iopub.status.idle":"2025-06-10T17:38:55.191780Z","shell.execute_reply.started":"2025-06-10T17:38:55.171346Z","shell.execute_reply":"2025-06-10T17:38:55.191043Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Remeber when we said we need to put \"z\" through some function to get \"Y\"? We have to make that function now. That function is sigmoid function. Its a function that maps real number values to 0 or 1. That means, we send some \"z\", we get some 0 or 1, and those values are \"y\".\n\nThe sigmoid function is defined as:\n\n$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n\n\nnow we are going to implement the function here.","metadata":{}},{"cell_type":"code","source":"def sigmoid(z):\n    return 1 / (1 + np.exp(-z))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:55.192612Z","iopub.execute_input":"2025-06-10T17:38:55.192938Z","iopub.status.idle":"2025-06-10T17:38:55.210655Z","shell.execute_reply.started":"2025-06-10T17:38:55.192908Z","shell.execute_reply":"2025-06-10T17:38:55.209841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, the weight and bias we initiated randomly, aren't necessarily accurate. We need to change that. In fact, we need to change it a lot of times till it gives us a line that's the perfect fit. If not, it should be near perfect. Also, the need to see the cost or the error. Error is the difference of the real value of y and the hypothetical value of y. Here, the cost function is \n$$J(w) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log a^{(i)} + (1 - y^{(i)}) \\log (1 - a^{(i)}) \\right]$$\nwhere:\n- \\( m \\) is the number of training examples.\n- \\( y^{(i)} \\) is the actual label (0 or 1).\n- \\( a^{(i)} \\) is the predicted probability from the sigmoid function.\n\nWe have to update the weights till we find the optimal weights. That means, we have to find the perfect or near perfect w1, w2 weights for variable x1, x2 to plug into the equation.\n\n### Gradient Descent Formula\n\nThe weight update rule in gradient descent is:\n\n$$ w := w - \\alpha \\frac{\\partial J}{\\partial w} $$\n\nwhere:\n- \\( w \\) represents the weights.\n- \\( \\alpha \\) is the learning rate.\n- \\( \\frac{\\partial J}{\\partial w} \\) is the gradient of the cost function with respect to \\( w \\).\n\nFor logistic regression, the gradient descent update for weights and bias is:\n\n$$ w := w - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} X^{(i)} (A^{(i)} - Y^{(i)}) $$\n\n$$ b := b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (A^{(i)} - Y^{(i)}) $$\n\nwhere:\n- \\( m \\) is the number of training examples.\n- \\( X^{(i)} \\) is the input feature vector.\n- \\( A^{(i)} \\) is the predicted probability from the sigmoid function.\n- \\( Y^{(i)} \\) is the actual label.\n\n\nWe will adjust the weights 1000 times. These attempts are called epochs. \n\n\n### What Happens in One Epoch: ###\n","metadata":{}},{"cell_type":"markdown","source":"*Forward Pass:* Compute predictions (A) and cost for all 20 examples.","metadata":{}},{"cell_type":"code","source":"def forward(X, w):\n    z = np.dot(X, w)\n    A = sigmoid(z)  # Predicted probabilities\n    J = -np.mean(y * np.log(A) + (1 - y) * np.log(1 - A))  # Cross-entropy cost\n    return A, J","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:55.211491Z","iopub.execute_input":"2025-06-10T17:38:55.211721Z","iopub.status.idle":"2025-06-10T17:38:55.228424Z","shell.execute_reply.started":"2025-06-10T17:38:55.211698Z","shell.execute_reply":"2025-06-10T17:38:55.227515Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Backward Pass:* Calculate the gradient (dw) by averaging contributions from all examples.","metadata":{}},{"cell_type":"code","source":"def backward(X, A, y):\n    dw = np.dot(X.T, (A - y)) / len(y)  # Gradient for weights\n    return dw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:55.229240Z","iopub.execute_input":"2025-06-10T17:38:55.229474Z","iopub.status.idle":"2025-06-10T17:38:55.245950Z","shell.execute_reply.started":"2025-06-10T17:38:55.229456Z","shell.execute_reply":"2025-06-10T17:38:55.245078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Update Weights:* Adjust w using the gradient: $$w = w - \\alpha * dw.$$\nHere, $$\\alpha$$ is the learning rate.\n\nWe have the liberty to choose the learning rate. But we should be careful, since with smaller learning rate, we need way more epochs and if we set it too big, we might cross the optimal point.","metadata":{}},{"cell_type":"code","source":"alpha = 0.01  # Learning rate\nepochs = 100\n\nfor epoch in range(epochs):\n    A, cost = forward(X_bias, w)  # Forward pass\n    dw = backward(X_bias, A, y)   # Backward pass\n    w -= alpha * dw               # Update weights\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, Cost: {cost:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:55.248544Z","iopub.execute_input":"2025-06-10T17:38:55.249310Z","iopub.status.idle":"2025-06-10T17:38:55.272178Z","shell.execute_reply.started":"2025-06-10T17:38:55.249278Z","shell.execute_reply":"2025-06-10T17:38:55.271330Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we check how good our hypothesis is after adjusting the weights **1,000** times!","metadata":{}},{"cell_type":"code","source":"def predict(X, w):\n    A, _ = forward(X, w)\n    return (A >= 0.5).astype(int)  # Threshold at 0.5\n\ny_pred = predict(X_bias, w)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:55.273086Z","iopub.execute_input":"2025-06-10T17:38:55.273546Z","iopub.status.idle":"2025-06-10T17:38:55.286123Z","shell.execute_reply.started":"2025-06-10T17:38:55.273519Z","shell.execute_reply":"2025-06-10T17:38:55.285198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accuracy = np.mean(y_pred == y)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:38:55.287316Z","iopub.execute_input":"2025-06-10T17:38:55.287613Z","iopub.status.idle":"2025-06-10T17:38:55.305679Z","shell.execute_reply.started":"2025-06-10T17:38:55.287588Z","shell.execute_reply":"2025-06-10T17:38:55.304686Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Honestly, it's not good. **Only 70.00% accuracy!**\nBut at least we learned something.\nLet's visually see  how our hypothesis worked.","metadata":{}},{"cell_type":"code","source":"# Fix Sleep_Hours at median (or mean)\nsleep_fixed = np.median(X[:, 1])  # Try np.mean() if needed\n\n# Create a range of Study_Hours values\nstudy_range = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)\n\n# Compute predicted probabilities (A) for each Study_Hours\nz = w[0] + w[1] * study_range + w[2] * sleep_fixed\nA = sigmoid(z)  # S-curve probabilities\n\n# Plot\nplt.figure(figsize=(5, 3))\nplt.plot(study_range, A, color='red', linewidth=3, label='Sigmoid Curve')\nplt.scatter(X[:, 0], y, color='blue', label='True Labels (0/1)')  # Actual data\nplt.xlabel('Study_Hours')\nplt.ylabel('Probability of Passing (A)')\nplt.title('S-shaped Sigmoid Curve: P(Pass) vs. Study_Hours')\nplt.axhline(y=0.5, color='gray', linestyle='--', label='Decision Threshold (A=0.5)')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:39:28.454132Z","iopub.execute_input":"2025-06-10T17:39:28.454429Z","iopub.status.idle":"2025-06-10T17:39:28.677518Z","shell.execute_reply.started":"2025-06-10T17:39:28.454407Z","shell.execute_reply":"2025-06-10T17:39:28.676627Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Real dataset ###\n\nAs the next step of our learning, we should see how to apply our knowledge on real life dataset.","metadata":{}},{"cell_type":"markdown","source":"This is dataset that consists of retina images and images that are not of retina. This could be a good dataset to train a model for binary classification.","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/skin-cancer-binary-classification-dataset/Skin_Data/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:01:34.158755Z","iopub.execute_input":"2025-06-19T20:01:34.163395Z","iopub.status.idle":"2025-06-19T20:01:34.177181Z","shell.execute_reply.started":"2025-06-19T20:01:34.163294Z","shell.execute_reply":"2025-06-19T20:01:34.175653Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import cv2\nimport pandas as pd\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:01:36.481494Z","iopub.execute_input":"2025-06-19T20:01:36.481836Z","iopub.status.idle":"2025-06-19T20:01:36.487145Z","shell.execute_reply.started":"2025-06-19T20:01:36.481814Z","shell.execute_reply":"2025-06-19T20:01:36.486158Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"labels = ['Cancer', 'Non_Cancer']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:01:38.445715Z","iopub.execute_input":"2025-06-19T20:01:38.446022Z","iopub.status.idle":"2025-06-19T20:01:38.451033Z","shell.execute_reply.started":"2025-06-19T20:01:38.446000Z","shell.execute_reply":"2025-06-19T20:01:38.450012Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"img_list=[]\nlabel_list =[]\nfor label in labels:\n    for img_file in os.listdir(path+label):\n        img_list.append(path+label+'/'+ img_file)\n        label_list.append(label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:02:28.231204Z","iopub.execute_input":"2025-06-19T20:02:28.231489Z","iopub.status.idle":"2025-06-19T20:02:28.281834Z","shell.execute_reply.started":"2025-06-19T20:02:28.231467Z","shell.execute_reply":"2025-06-19T20:02:28.280885Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"label_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:03:12.601660Z","iopub.execute_input":"2025-06-19T20:03:12.602414Z","iopub.status.idle":"2025-06-19T20:03:12.608005Z","shell.execute_reply.started":"2025-06-19T20:03:12.602385Z","shell.execute_reply":"2025-06-19T20:03:12.607108Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"['Cancer', 'Cancer', 'Non_Cancer', 'Non_Cancer']"},"metadata":{}}],"execution_count":25}]}